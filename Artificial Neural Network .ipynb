{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "# Installing Keras\n",
    "# Enter the following command in a terminal (or anaconda prompt for Windows users): conda install -c conda-forge keras\n",
    "\n",
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv(r'C:\\Users\\akiper\\Desktop\\Machine Learning Course\\Machine-Learning-A-Z-New\\Machine Learning A-Z New\\Part 8 - Deep Learning\\Section 39 - Artificial Neural Networks (ANN)\\Churn_Modelling.csv')\n",
    "x = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the \"Gender\" Column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "x[:, 2] = le.fit_transform(x[:,2]) #Applying the encoding specifically to the Gender column in the X matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the \"Geography\" Column\n",
    "#Note: We have to use OneHot Encoding because there is no order of relationship between the countries (Gender is binary so regular encoding works there but not here)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "x = np.array(ct.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into Traning and Test Set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "#Note: Feature Scaling is REQUIRED for Deep Learning and building ANN's\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the ANN\n",
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the input layer and the 1st hidden layer\n",
    "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))\n",
    "#Dense class: Creates a fully connected layer\n",
    "#Units is the number of hidden neurons want to have in connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the 2nd layer\n",
    "ann.add(tf.keras.layers.Dense(units = 6, activation = 'relu')) #Copy and Paste the previous cell's code to create another layer (i.e. this 2nd layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the Output layer\n",
    "ann.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid')) #NOTE: when dealing with binary outcomes, you only need 1 unit; When dealing with non-binary outputs, then use the actual number of output choices\n",
    "#NOTE: For output layer, you want to use the Sigmoid Activation Funciton bc it gives us the probablity that the outcome is accurate along with the predicted outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the ANN\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s 83us/sample - loss: 0.6565 - accuracy: 0.6237\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.4779 - accuracy: 0.7994\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.4327 - accuracy: 0.8114\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.4132 - accuracy: 0.8170\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3992 - accuracy: 0.8227\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3872 - accuracy: 0.8324\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3778 - accuracy: 0.8380\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3707 - accuracy: 0.8449\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.3650 - accuracy: 0.8474\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3607 - accuracy: 0.8511\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3576 - accuracy: 0.8526\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3554 - accuracy: 0.8521\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.3532 - accuracy: 0.8565\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.3518 - accuracy: 0.8560\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.3502 - accuracy: 0.8576\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s 35us/sample - loss: 0.3489 - accuracy: 0.8564\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3478 - accuracy: 0.8585\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3467 - accuracy: 0.8579\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3458 - accuracy: 0.8591\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3447 - accuracy: 0.8590\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3442 - accuracy: 0.8595\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3438 - accuracy: 0.8600\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3431 - accuracy: 0.8600\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3424 - accuracy: 0.8602\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3416 - accuracy: 0.8600\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3416 - accuracy: 0.8616\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3412 - accuracy: 0.8602\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3409 - accuracy: 0.8615\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3407 - accuracy: 0.8609\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3402 - accuracy: 0.8615\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3401 - accuracy: 0.8621\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3394 - accuracy: 0.8612\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3395 - accuracy: 0.8626\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3390 - accuracy: 0.8626\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s 43us/sample - loss: 0.3390 - accuracy: 0.8626\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3385 - accuracy: 0.8619\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3385 - accuracy: 0.8625\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3384 - accuracy: 0.8611\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3382 - accuracy: 0.8636\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3381 - accuracy: 0.8626\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3379 - accuracy: 0.8614\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3376 - accuracy: 0.8634\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 0.3379 - accuracy: 0.8614\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3375 - accuracy: 0.8627\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3376 - accuracy: 0.8615\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3375 - accuracy: 0.8636\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3372 - accuracy: 0.8620\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3372 - accuracy: 0.8615\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3376 - accuracy: 0.8626\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3370 - accuracy: 0.8639\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3371 - accuracy: 0.8620\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 0.3369 - accuracy: 0.8626\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3370 - accuracy: 0.8644\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3368 - accuracy: 0.8624\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3369 - accuracy: 0.8633\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3368 - accuracy: 0.8634\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3365 - accuracy: 0.8622\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3366 - accuracy: 0.8645\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3369 - accuracy: 0.8641\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s 37us/sample - loss: 0.3366 - accuracy: 0.8630\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3365 - accuracy: 0.8641\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3365 - accuracy: 0.8637\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s 40us/sample - loss: 0.3360 - accuracy: 0.8641\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s 38us/sample - loss: 0.3361 - accuracy: 0.8636\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 0.3366 - accuracy: 0.8648\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3362 - accuracy: 0.8643s - loss: 0.3270 - accura\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3361 - accuracy: 0.8643\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3362 - accuracy: 0.8645\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s 36us/sample - loss: 0.3362 - accuracy: 0.8629\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3360 - accuracy: 0.8640\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s 27us/sample - loss: 0.3359 - accuracy: 0.8652\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s 28us/sample - loss: 0.3359 - accuracy: 0.8630\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3361 - accuracy: 0.8645\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s 29us/sample - loss: 0.3359 - accuracy: 0.8631\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3355 - accuracy: 0.8635\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3360 - accuracy: 0.8651\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s 39us/sample - loss: 0.3357 - accuracy: 0.8639\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s 29us/sample - loss: 0.3356 - accuracy: 0.8649\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3355 - accuracy: 0.8650\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3354 - accuracy: 0.8649\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3356 - accuracy: 0.8624\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3354 - accuracy: 0.8634\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 0.3356 - accuracy: 0.8648\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s 45us/sample - loss: 0.3355 - accuracy: 0.8635\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3354 - accuracy: 0.8630\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 0.3353 - accuracy: 0.8637\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s 29us/sample - loss: 0.3348 - accuracy: 0.8652\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3350 - accuracy: 0.8643\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3349 - accuracy: 0.8660\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s 29us/sample - loss: 0.3350 - accuracy: 0.8639\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s 29us/sample - loss: 0.3348 - accuracy: 0.8652s - loss: 0.3371 - accuracy\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3347 - accuracy: 0.8654\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3349 - accuracy: 0.8648\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s 30us/sample - loss: 0.3348 - accuracy: 0.8640\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s 34us/sample - loss: 0.3345 - accuracy: 0.8646\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3344 - accuracy: 0.8640\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s 33us/sample - loss: 0.3346 - accuracy: 0.8636\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3348 - accuracy: 0.8635\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s 32us/sample - loss: 0.3346 - accuracy: 0.8625\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s 31us/sample - loss: 0.3344 - accuracy: 0.8635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2336a403208>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the ANN on the Training set\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the result of a single observation\n",
    "print(ann.predict(x = sc_x.transform([[1,0,0,600,1,40,3, 60000, 2, 1, 1, 50000]])) > 0.5) #Note: use '>0.5' to change result from probablility to word result\n",
    "#NOTE: With One Hot Encoding, you must separate the values with a comma even though they are not separated by a comma in the print result above\n",
    "#NOTE: Remember, apply the predict method onto the scale/scaling used in feature scaling (i.e. use sc object from feature scaling section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " ...\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the Test set Results\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred = (y_pred > 0.5) #If y_pred < 0.5, then y pred will become 0 (False) and if y pred> 0.5, then y pred becomes 1 (True)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n",
    "#On left is predicted outcome and right is actual outcome "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1516   79]\n",
      " [ 199  206]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.861"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "print(matrix)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
