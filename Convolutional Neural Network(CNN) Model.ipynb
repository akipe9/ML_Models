{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data Preprocessing\n",
    "#Preprocessing the Training Set\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, #Applies feature scaling to each pizels by dividing them all by 255; Feature Scaling required for training neural networks\n",
    "                                   shear_range = 0.2, #geometrically alters image\n",
    "                                   zoom_range = 0.2, #Zooms into image\n",
    "                                   horizontal_flip = True) #Flips image\n",
    "#This step reduces overfitting of training step/data\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(r'C:\\Users\\akiper\\Desktop\\Machine Learning Course\\Machine-Learning-A-Z-New\\Machine Learning A-Z New\\Part 8 - Deep Learning\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\training_set',\n",
    "                                                 target_size = (64, 64), #Final size of images when they are fed into CNN; NOTE: larger sizes makes the training longer\n",
    "                                                 batch_size = 32, #How many images we want in each batch; NOTE: 32 is typically default size\n",
    "                                                 class_mode = 'binary') #Either Binary or Categorical; Binary in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing the Test Set\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255) #We  just want to apply feature scaling to the pixels of these images as they are treated as new images different than the training set images. We do NOT want to use zoom, flip, shear, transformations on test set as they will create new images. \n",
    "test_set = test_datagen.flow_from_directory(r'C:\\Users\\akiper\\Desktop\\Machine Learning Course\\Machine-Learning-A-Z-New\\Machine Learning A-Z New\\Part 8 - Deep Learning\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\test_set', #Imports test set images into our notebook\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Building the CNN\n",
    "#Initializing the CNN\n",
    "cnn = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = (64,64,3)))\n",
    "#Filters: # of Feature Detectors that you want to apply to your images; aka features or kernels\n",
    "#Kernel Size: size of pixels/dimesnsions NOTE: number chosen is n x n dimensions (i.e. 3 =  3x3 dimensions)\n",
    "#Activation: Type of Activation FUnction to use: Relu - rectifier\n",
    "#Input_Shape: resize of input images; NOTE: last number determines if images are in color or b/w; 1 = b/w and 3 = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))\n",
    "#Max Pooling is found in Max Pool 2D class NOTE: number chosen is n x n dimensions (i.e. 3 =  3x3 dimensions)\n",
    "#Pool Size: specify the max pizel size of the pixels found in the feature map during the Max Pooling process; \n",
    "#Recommended stride(how many spaces the feature detector moves across feature map):2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a Second Convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 3, activation = 'relu', input_shape = (64,64,3)))#Input_Shape used ONLY when 1st layer is added; connects 1st layer to input layer, which then adds input layer\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2, strides = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units = 128, activation = 'relu'))\n",
    "#Dense class: Creates a fully connected layer\n",
    "#Units is the number of hidden neurons want to have in connected layer; NOTE: larger number may equal to more accurate results\n",
    "#NOTE: As long as we have not reached final output layer, use Rectifier Activation Function (ReLu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "#Only need 1 neuron because we are doing a binary classification task\n",
    "#Because we are doing a binary classification task, Sigmoid Activation Function must be used. Else, Softmax Activation Function is used in multi-class classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Training the CNN \n",
    "#Compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "#Compile: \n",
    "#Adam Optimizer: Performs Stochastic Gradient Descent to update the weights to reduce loss error between predictions and targets\n",
    "#Loss: Binary_CrossEntropy becaues we are doing a binary task\n",
    "#Metrics: Accuracy is most relevant way to measure performance of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 250 steps, validate for 63 steps\n",
      "Epoch 1/25\n",
      "250/250 [==============================] - 72s 286ms/step - loss: 0.6902 - accuracy: 0.5505 - val_loss: 0.6744 - val_accuracy: 0.5825\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 71s 283ms/step - loss: 0.6384 - accuracy: 0.6381 - val_loss: 0.6243 - val_accuracy: 0.6495\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 68s 274ms/step - loss: 0.6125 - accuracy: 0.6727 - val_loss: 0.5785 - val_accuracy: 0.7025\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 66s 262ms/step - loss: 0.5684 - accuracy: 0.7091 - val_loss: 0.5296 - val_accuracy: 0.7525\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 61s 244ms/step - loss: 0.5341 - accuracy: 0.7339 - val_loss: 0.5305 - val_accuracy: 0.7385\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 56s 223ms/step - loss: 0.5059 - accuracy: 0.7496 - val_loss: 0.5406 - val_accuracy: 0.7505\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 0.4907 - accuracy: 0.7620 - val_loss: 0.4978 - val_accuracy: 0.7575\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.4796 - accuracy: 0.7684 - val_loss: 0.5115 - val_accuracy: 0.7615\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 55s 221ms/step - loss: 0.4628 - accuracy: 0.7789 - val_loss: 0.4994 - val_accuracy: 0.7565\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 57s 227ms/step - loss: 0.4478 - accuracy: 0.7884 - val_loss: 0.4916 - val_accuracy: 0.7610\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 69s 278ms/step - loss: 0.4400 - accuracy: 0.7905 - val_loss: 0.4607 - val_accuracy: 0.7755\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.4249 - accuracy: 0.8011 - val_loss: 0.4709 - val_accuracy: 0.7765\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 66s 266ms/step - loss: 0.4146 - accuracy: 0.8073 - val_loss: 0.4621 - val_accuracy: 0.8025\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 63s 253ms/step - loss: 0.3951 - accuracy: 0.8207 - val_loss: 0.4573 - val_accuracy: 0.7960\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.3831 - accuracy: 0.8257 - val_loss: 0.4862 - val_accuracy: 0.7830\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 66s 263ms/step - loss: 0.3797 - accuracy: 0.8257 - val_loss: 0.5225 - val_accuracy: 0.7550\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 62s 248ms/step - loss: 0.3752 - accuracy: 0.8319 - val_loss: 0.4965 - val_accuracy: 0.7820\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.3525 - accuracy: 0.8429 - val_loss: 0.4640 - val_accuracy: 0.7975\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 58s 232ms/step - loss: 0.3382 - accuracy: 0.8525 - val_loss: 0.4912 - val_accuracy: 0.7905\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 65s 261ms/step - loss: 0.3298 - accuracy: 0.8590 - val_loss: 0.4753 - val_accuracy: 0.8010\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 67s 270ms/step - loss: 0.3179 - accuracy: 0.8570 - val_loss: 0.4804 - val_accuracy: 0.7900\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 72s 286ms/step - loss: 0.3032 - accuracy: 0.8694 - val_loss: 0.4665 - val_accuracy: 0.8070\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 63s 252ms/step - loss: 0.2980 - accuracy: 0.8720 - val_loss: 0.5052 - val_accuracy: 0.7875\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 62s 249ms/step - loss: 0.2875 - accuracy: 0.8758 - val_loss: 0.4765 - val_accuracy: 0.7935\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 61s 244ms/step - loss: 0.2827 - accuracy: 0.8794 - val_loss: 0.4830 - val_accuracy: 0.8130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2768e48e888>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the CNN on the Training Set and evaluating it on the Test Set\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)\n",
    "#Validation_data: test set that we will evaluate performance of model on; Just has been feature-scaled, no other transformations\n",
    "#Epochs: number of epochs; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 4: Make a single prediction\n",
    "import numpy as np\n",
    "#from tensorflow import image\n",
    "test_image = tf.keras.preprocessing.image.load_img(r'C:\\Users\\akiper\\Desktop\\Machine Learning Course\\Machine-Learning-A-Z-New\\Machine Learning A-Z New\\Part 8 - Deep Learning\\Section 40 - Convolutional Neural Networks (CNN)\\dataset\\single_prediction\\cat_or_dog_4.jpg', target_size = (64, 64)) #NOTE: target size = pixel size \n",
    "test_image = tf.keras.preprocessing.image.img_to_array(test_image)#Converts img into Numpy array, which is what prediction model expects \n",
    "test_image = np.expand_dims(test_image, axis = 0)#Handles single input as batch since we trained the model using batches\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices #Determines which output corresponds to 0 and 1 (because we are doing a binary output task)\n",
    "if result[0][0] == 1:#1st bracket indicates batch number (0 indicates 1st batch) 2nd bracket indicates image number in batch(0 indicates 1st image)\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
